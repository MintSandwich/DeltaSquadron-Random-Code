{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwRkjThLVNUs+BElkYbtPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlvmalvinn/DeltaSquadron-Random-Code/blob/main/ArtNaon_Model_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Locate The Data Path"
      ],
      "metadata": {
        "id": "0H75AuMCscjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get the Google drive Path Location, with google sign in"
      ],
      "metadata": {
        "id": "YMwbn6e3sUqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TKxNs6-sOkx"
      },
      "outputs": [],
      "source": [
        "# Google.colab drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the desired data location to the current working directory\n",
        "%cd /content/drive/MyDrive/Dataset"
      ],
      "metadata": {
        "id": "PWfo7a25spbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras prepocessing library\n",
        "! pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "3tVQafkZsrGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the required Library\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import ResNet50V2, VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, MaxPool2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Lza2NBNztAsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting The Dataset"
      ],
      "metadata": {
        "id": "4D1yukA0tKbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library to split datasets\n",
        "! pip install split_folders"
      ],
      "metadata": {
        "id": "ZLY26wORtRQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Dataset's Folder path and the splitted Data's target location\n",
        "\n",
        "import splitfolders # or import splitfolders\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Art/NoDuplicates/\" # Change The path depending on your file\n",
        "\n",
        "# where you want the split datasets saved. one will be created if it does not exist or none is set\n",
        "output = \"/content/drive/MyDrive/Dataset/SplitData\" # Change The path depending on your file\n",
        "\n",
        "# ratio of split are in order of train/val/test. You can change to whatever you want. For train/val sets only, you could do .75, .25 for example.\n",
        "splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .2))"
      ],
      "metadata": {
        "id": "-0hU3efxtXqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the library required\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training Data Path and Image gen (Normalization)\n",
        "TRAINING_DIR = \"/content/drive/MyDrive/Dataset/SplitData/train\" # Change The path depending on your file\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Validation Data Path and Image gen (Normalization)\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/Dataset/SplitData/val\" # Change The path depending on your file\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# Training gen and batching\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(224,224),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=32\n",
        ")\n",
        "\n",
        "# Validation gen and batching\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(224,224),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "pS7Sa9sLuGUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the File's in the Directory"
      ],
      "metadata": {
        "id": "IJjWolEnyTX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just making sure the library to import things from G Drive Is present\n",
        "from google.colab import drive\n",
        "import os # Library to manipulate OS  I.E Copying, changing, and or reading things on folders\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataset/SplitData/train/' # Change directory depending on your file's path\n",
        "\n",
        "# List the contents of the directory\n",
        "# Code to Check wether there is file's in the directory or not (Might be useful to debug)\n",
        "try:\n",
        "    files = os.listdir(directory)\n",
        "    print(\"Files in directory:\", files)\n",
        "except FileNotFoundError:\n",
        "    print(f\"The directory {directory} does not exist.\")"
      ],
      "metadata": {
        "id": "6s6cTMmMxSq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If the file's extension is not .JPG, Use this code"
      ],
      "metadata": {
        "id": "1b-Zh-SGyjFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def check_and_convert_to_jpeg(directory):\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "\n",
        "                    # Check if the file is in WEBP format\n",
        "                    if img.format == 'WEBP': # Set this to What extension you want to change to JPEG / JPG\n",
        "\n",
        "                        # Convert to JPEG\n",
        "                        new_file_path = os.path.splitext(file_path)[0] + '.jpg'\n",
        "                        img = img.convert('RGB')  # Convert to RGB to ensure JPEG compatibility\n",
        "                        img.save(new_file_path, 'JPEG')\n",
        "                        print(f\"Converted {file_path} to {new_file_path}\")\n",
        "                        os.remove(file_path)  # Remove the original WEBP file / the extension that you set\n",
        "\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Cannot open file: {file_path} ({e})\")\n",
        "\n",
        "# Check and convert files in the training data directory\n",
        "train_directory = \"/content/drive/MyDrive/Dataset/SplitData/train\"\n",
        "check_and_convert_to_jpeg(train_directory)\n",
        "\n",
        "# Check and convert files in the validation data directory\n",
        "val_directory = \"/content/drive/MyDrive/Dataset/SplitData/val\"\n",
        "check_and_convert_to_jpeg(val_directory)"
      ],
      "metadata": {
        "id": "t0C2Ot5vysHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Dataset Onto The Train and Val Variable"
      ],
      "metadata": {
        "id": "NT2bzCiRyE4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"/content/drive/MyDrive/Dataset/SplitData/train\",\n",
        "  image_size=(224, 224),\n",
        "  batch_size=32,\n",
        "  seed=123\n",
        ")\n",
        "\n",
        "# Load validation data\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"/content/drive/MyDrive/Dataset/SplitData/val\",\n",
        "  image_size=(224, 224),\n",
        "  batch_size=32,\n",
        "  seed=123\n",
        ")"
      ],
      "metadata": {
        "id": "RZ_HqSL5yL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Attempt 1"
      ],
      "metadata": {
        "id": "FRMyIdo_zJx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The Machine Learning Model\n",
        "model1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Conv2D(16,(3,3),activation = \"relu\" , input_shape = (224,224,3)) ,\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\") ,\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128,(3,3),activation = \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "5uEON0SNzNzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile The Model\n",
        "model1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IkDDXmGVzRWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit The Data Onto the previously compiled Model\n",
        "model1.fit(train_data,\n",
        "          validation_data=val_data,\n",
        "          epochs=10) #Change The Epoch (Training Attempt / Loop) Depending On your Judgement"
      ],
      "metadata": {
        "id": "ozhcVL4dzdeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Attempt 2"
      ],
      "metadata": {
        "id": "fcT0K1qTz0RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lc4pMGp00CaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}